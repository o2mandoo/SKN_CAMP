{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "446aa08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths= (\"https://news.naver.com/section/101\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"sa_text\", \"sa_item_SECTION_HEADLINE\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f92c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84b3841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\"k\" : 1, \"fetch_k\" : 4}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bd15411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://news.naver.com/section/101'}, page_content='이데일리\\n\\n18분전\\n\\n\\n\\n\\n\\n\\n\\n\\n[아침밥] 하나증권 \"롯데관광개발, 2분기 사상 최대 실적… 목표가 쭉\"'),\n",
       " Document(metadata={'source': 'https://news.naver.com/section/101'}, page_content=\"에이피알(278470)이 1분기에 이어 2분기에도 신기록을 세우며 '연 매출 1조 원' 목표에 성큼 다가섰다. 에이피알은 2분기 경영실적을 잠정 집계한 결과 연결 기준 매출액이 3277억 원으로 전년 동기 대비 11\\n\\n\\n뉴스1\\n\\n27분전\"),\n",
       " Document(metadata={'source': 'https://news.naver.com/section/101'}, page_content='한국경제\\n\\n14분전\\n\\n\\n\\n\\n\\n\\n\\n\\n‘마스가’ 수혜 기대감↑…미래에셋, ‘TIGER 조선TOP10 ETF’ 순자산 4000억원 돌파 [투자360]')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever],\n",
    "                weights=[0.2, 0.8])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ensemble_retriever.invoke(\"오늘의 증시\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb61d86",
   "metadata": {},
   "source": [
    "서로 다른 질문을 물어보고, 지능적으로 결합 후 그중 순위를 만들어 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b7f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ad7c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "당신은 AI 언어 모델 조수입니다. 당신의 임무는 주어진 사용자 질문에 대해 벡터 데이터베이스에서 관련 문서를 검색할 수 있도록 다섯 가지 다른 버전을 생성하는 것입니다.\n",
    "사용자 질문에 대한 여러 관점을 생성함으로써, 거리 기반 유사성 검색의 한계를 극복하는 데 도움을 주는 것이 목표입니다.\n",
    "각 질문은 새 줄로 구분하여 제공하세요. 원본 질문: {question}\n",
    "\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOpenAI(model_name=\"chatgpt-4o-latest\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x : x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain.load import dumps, loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b002351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results, k=60, top_n=2):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(docs)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "   \n",
    "    reranked_results = [ (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x : x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ee7f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = generate_queries | ensemble_retriever.map() | reciprocal_rank_fusion\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "template = \"\"\"다음 맥락을 바탕으로 질문에 답변할 것\n",
    "{context}\n",
    "\n",
    "\n",
    "질문: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0302c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = (\n",
    "    {\n",
    "        \"context\" : chain,\n",
    "        \"question\" : RunnablePassthrough()\n",
    "    }\n",
    "    | prompt | ChatOpenAI(model_name=\"chatgpt-4o-latest\", temperature=0)\n",
    "    | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6b7fb",
   "metadata": {},
   "source": [
    "OpenAI 오디오 스피치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54d0f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13568/814652417.py:11: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  reranked_results = [ (loads(doc), score)\n",
      "/tmp/ipykernel_13568/1925078889.py:15: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(\"./output.mp3\")\n"
     ]
    }
   ],
   "source": [
    "rt = final_chain.invoke(\"오늘의 증시\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"onyx\",\n",
    "    input=rt\n",
    "    )\n",
    "\n",
    "\n",
    "response.stream_to_file(\"./output.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c434e",
   "metadata": {},
   "source": [
    "###  pdf 에서 텍스트 추출 후 Rag에 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491bd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4cb26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_elements(path, fname):\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=True,  # PDF에서 이미지를 추출\n",
    "        infer_table_structure=True,  # 테이블 구조를 추론\n",
    "        chunking_strategy=\"by_title\",  # 타이틀을 기준으로 텍스트를 블록으로 분할\n",
    "        max_characters=4000,  # 최대 4000자로 텍스트 블록을 제한\n",
    "        new_after_n_chars=3800,  # 3800자 이후에 새로운 블록 생성\n",
    "        combine_text_under_n_chars=2000,  # 2000자 이하의 텍스트는 결합\n",
    "        image_output_dir_path=path,  # 이미지가 저장될 경로 설정\n",
    "        languages=['kor']\n",
    "        # image_output_dir_path=os.path.join(os.getcwd(),\"figures\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5e03723",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = extract_pdf_elements(\"./\", \"RRF.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9393b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    PDF에서 추출한 요소들을 테이블과 텍스트로 분류합니다.\n",
    "    raw_pdf_elements: unstructured.documents.elements 리스트\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))  # 테이블 요소를 저장\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))  # 텍스트 요소를 저장\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "texts, tables = categorize_elements(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c9b16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['띠 여러 개의 서로 다 는 ㄴㄴ [라간 ( 목 6000『0081 『<801<『46100) 하여 하 나 의 니다. 퓨 전 (『48100) 알고리즘 중 으 브 만드는 알고리즘 매긴 점 수 (6006) 가 아 있습니다.\\n\\n00 101 0 어 , 81125 를 들 근 크 여 을 때 사 용 됩니다. 결 과 를 만들고\\n\\n따 간의 핵심 원리: 역 순 위 (60000 『8010\\n\\n심은 역 순 위 (\\\\6 어 0『0 ㅇ ㅇ 801 800) 개 념 에 있습니다. 의 따 간의 이\\n\\n1/7( 마 에 있다면, 아 가 에서 번째 41 시 연 역 + 0) 가 됩니다.\\n\\n서 의 순위 ((3010) 므 느 :\\n\\n} 수 (보통 60 사용) 6: 순 위 가 너무 높아서 점 수 가 과 도 하게 커지\\n\\n과 같이 점 수 가 매 겨 집니다. 이는 상 전 런 … 1/62 느 ㄴ 1/61 점 , 2 위 느 ㄴ 니다. 1 위 } 번 힌\\n\\n(5[60-0+「-5[60) <! ㅁ 0 내 0 깨 [ [ 따라\\n\\n니다. 101 『 쥐 [ 1 / (< + 603) 를 계\\n\\n0 \\\\ ㅁ 하 라 85 이 이 ~ 마 해당 으로 나타나는 문 서 가 있다면\\n\\n마란 예제\\n\\n두 개의 검색 시 스 템 (&, 6) 이 3 개 의 문 서 (0001, 0062, 0063) 에 대해 다 음 과 같 해 봅시다. (6=60 사용) [10 다 그 0\\n\\n검색 시스템 4 의 결과\\n\\n0001\\n\\n0003\\n\\n검색 시스템 6 의 결과\\n\\n0001\\n\\n스템 & 에 서 1 위 : 1 / (1 + 60) =0.0164\\n\\n스템 8 에 서 2 위 : 1 / (2 + 60) =0.0161\\n\\n0002:\\n\\n스템 & 에 서 순위 없음: 0 점\\n\\n스템 8 에 서 1 위 : 1 / (1 + 60) =0.0164\\n\\n0003:\\n\\n스템 & 에 서 2 위 : 1 / (2 + 60) =0.0161\\n\\n스템 6 에 서 순위 없음: 0 점\\n\\n2. 점수 합산\\n\\n0001 종점: 0.0164 + 0.0161 = 0.0325\\n\\n0002 총점: 0 + 0.0164 = 0.0164\\n\\n0003 종점: 0.0161 + 0 = 0.0161\\n\\n위 3. 최 얘 0>\\n\\n0001 (0.0325 점 ) 0002 (0.0164 점 ) 0003 (0.0161 점 )\\n\\n따 간의 장점\\n\\n스 하 . 단순함: [ 우 고 리 즘 이 매우 간 단 하고 직 관 적 입니다.\\n\\n정규화 불필요: 각 시 스 템 의 점수 범 위 ( 스 케 일 ) 가 달라도 상 관 없습니다. 오직 때문에 점 수 를 정 규 화 (\\\\ 야 머 0129000) 할 필 요 가 없습니다. 위 정 보 만 사 용 하기 다\\n\\n견 고 함 (#064680065): 특정 검색 시스템 하 나 의 성 능 이 냐 쁘 더라도, 다른 시 스 템 들의 순위 정 보 를 통해 그 영 향 을 완 화 할 수 있습니다. 한 시 스 템 에서 매우 낮은 순 위 를 받은 문 서 라 도 다른 여러 시스 템 에서 상 위 권 을 차 지 했다면 최종 순 위 가 높아질 수 있습니다.\\n\\n러한 특징 때문에 | 간 간 는 하이브리드 검 색 (|1/600 568「0) 처 럼 여러 검색 기 술 을 결 합 하여 전반적인 색 품 질 을 높 이 고 자 할 때 매우 유 용 하게 사 용 됩니다.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "696bc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "341c59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_texts = \"\".join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11ca615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token = text_splitter.split_text(joined_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5d99b",
   "metadata": {},
   "source": [
    "### 로드한 문서 데이터를 요약하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b6826d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3368b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "당신은 표와 텍스트를 요약해 검색에 활용할 수 있도록 도와주는 도우미 입니다.\n",
    "이 요약본들은 임베딩되어 원본 ㄴ텍스트나 표 요소를 검색하는데 사용될 것입니다.\n",
    "주어진 표나 텍스트의 내용을 검색에 최적화된 간결한 요약으로 작성하세요.\n",
    "요약할 표 또는 텍스트 : {element}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71fbec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "model = ChatOpenAI(model_name=\"chatgpt-4o-latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b793f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_chain = (\n",
    "    {\n",
    "        'element' : lambda x : x\n",
    "    }\n",
    "    | prompt | model | StrOutputParser()\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "text_summaries = summaries_chain.batch(texts, {'max_concurrency' : 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "276dbfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['요약:  \\n이 문서는 여러 검색 시스템의 결과를 통합하는 알고리즘인 Reciprocal Rank Fusion (RRF)에 대해 설명한다. RRF는 각 문서의 순위에 따라 1 / (k + rank) 방식으로 점수를 부여하고, 여러 시스템에서의 점수를 합산하여 최종 순위를 결정한다. 이 방식은 단순하고 직관적이며, 점수 정규화가 필요 없고, 특정 시스템의 성능 저하에도 견고하게 작동한다. 하이브리드 검색 시스템에서 효과적으로 사용된다.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca325cc",
   "metadata": {},
   "source": [
    "## 데이터셋을 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f46231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# 허깅페이스 fasionpedia 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a437501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./fashion_dataset.zip\", 'r') as f:\n",
    "    f.extractall(\"./fashion/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c77b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import chromadb\n",
    "\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\"./fvector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48242412",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = ImageLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6f46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = OpenCLIPEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd353db",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vdb = chroma_client.get_or_create_collection(\n",
    "        name=\"image\", embedding_function=CLIP, data_loader=image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c706ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ids = []\n",
    "uris =[]\n",
    "roots = \"./fashion/fashion_dataset/\"\n",
    "for idx, filename in enumerate(sorted(os.listdir(roots))):\n",
    "    if filename.endswith(\".png\"):\n",
    "        uris.append(roots+filename)\n",
    "        ids.append(str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a18a0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vdb.add(ids=ids, uris=uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae0a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def translate(text):\n",
    "    model  = ChatOpenAI(model='gpt-4.1', temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('system', \"You are a translator. Translate the following text to English\"),\n",
    "            ('user', \"{text}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({'text' : text})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = translate(\"검은색 티셔츠와 청바지 추천\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ac7d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(image_vdb, query, results=2):\n",
    "    # 주어진 쿼리를 실행하고, 상위 결과 반환\n",
    "    return image_vdb.query(\n",
    "        query_texts=[query],\n",
    "        n_results=results,\n",
    "        include=['uris', 'distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c0555d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = image_vdb.query(\n",
    "    query_texts=[query],\n",
    "    n_results=2,\n",
    "    include=['uris', 'distances']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chain():\n",
    "    gpt4 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    parser = StrOutputParser()\n",
    "    image_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful fashion and styling assistant. Answer the user's question using the given image context with direct references to parts of the images provided. Maintain a more conversational tone, don't make too many lists. Use markdown formatting for highlights, emphasis, and structure.\"),\n",
    "            (\"user\", [\n",
    "                {\"type\": \"text\", \"text\": \"What are some ideas for styling {user_query}\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": \"data:image/jpeg;base64,{image_data_1}\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": \"data:image/jpeg;base64,{image_data_2}\"},\n",
    "            ]),\n",
    "        ])\n",
    "\n",
    "\n",
    "    return image_prompt | gpt4 | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb2d31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_inputs(data, user_query):\n",
    "    inputs = {}\n",
    "\n",
    "\n",
    "    inputs['user_queyr'] = user_query\n",
    "\n",
    "\n",
    "    image_path_1 = data['uris'][0][0]\n",
    "    image_path_2 = data['uris'][0][1]\n",
    "\n",
    "\n",
    "    with open(image_path_1, 'rb') as image_file:\n",
    "        image_data_1 = image_file.read()\n",
    "    inputs['image_data_1'] = base64.b64encode(image_data_1).decode('utf-8')\n",
    "\n",
    "\n",
    "    with open(image_path_2, 'rb') as image_file:\n",
    "        image_data_2 = image_file.read()\n",
    "    inputs['image_data_2'] = base64.b64encode(image_data_2).decode('utf-8')\n",
    "\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4c9d93a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vision_chain' has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m results = query_db(image_vdb, en_query)\n\u001b[32m      5\u001b[39m prompt_input = format_prompt_inputs(results, en_query)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response_en = \u001b[43mvision_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m(prompt_input)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'vision_chain' has no attribute 'invoke'"
     ]
    }
   ],
   "source": [
    "ko_query = \"바지하고 티셔츠를 파란색으로 추천\"\n",
    "en_query = translate(ko_query)\n",
    "\n",
    "results = query_db(image_vdb, en_query)\n",
    "prompt_input = format_prompt_inputs(results, en_query)\n",
    "response_en = vision_chain.invoke(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce10e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 기반 패션 추천 파이프라인: 한글 질문→영어 번역→유사 이미지 DB 검색→이미지+영문질문 LLM 프롬프트→답변 생성→최종 한글 번역까지 엔드 투 엔드\n",
    "import base64                                  # 이미지 파일 base64 인코딩용\n",
    "from langchain_openai import ChatOpenAI         # LLM(챗GPT-4o 등)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def query_db(image_vdb, query, results=2):\n",
    "    # 헤드라이너\n",
    "    # 이미지 벡터DB(image_vdb)에 사용자(영어) 쿼리로 top-N 유사 이미지의 경로/유사도 추출\n",
    "    return image_vdb.query(\n",
    "        query_texts=[query],           # (리스트 형태 english query)\n",
    "        n_results=results,             # 상위 N개 이미지 반환\n",
    "        include=['uris', 'distances']  # 반환: 이미지 경로와 유사도 점수\n",
    "    )\n",
    "\n",
    "# 예시: 직접 DB 쿼리 가능(일반적으론 아래 함수로 래핑해서 사용)\n",
    "# tmp = image_vdb.query(query_texts=[query], n_results=2, include=['uris', 'distances'])\n",
    "\n",
    "def setup_chain():\n",
    "    # LLM 프롬프트+모델+파서 구성: 자연어 '스타일링 아이디어' 생성용\n",
    "    gpt4 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    parser = StrOutputParser()\n",
    "    image_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \n",
    "         \"You are a helpful fashion and styling assistant. Answer the user's question using the given image context with direct references to parts of the images provided. Maintain a more conversational tone, don't make too many lists. Use markdown formatting for highlights, emphasis, and structure.\"\n",
    "        ),\n",
    "        (\"user\", [\n",
    "            {\"type\": \"text\", \"text\": \"What are some ideas for styling {user_query}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": \"data:image/jpeg;base64,{image_data_1}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": \"data:image/jpeg;base64,{image_data_2}\"},\n",
    "        ]),\n",
    "    ])\n",
    "    return image_prompt | gpt4 | parser\n",
    "\n",
    "def format_prompt_inputs(data, user_query):\n",
    "    # DB 쿼리 결과(data: dict)와 사용자(영문) 질문을 LLM 입력 dict로 가공\n",
    "    inputs = {}\n",
    "    inputs['user_query'] = user_query\n",
    "    # 유사도 상위 이미지 2개의 경로 추출\n",
    "    image_path_1 = data['uris'][0][0]\n",
    "    image_path_2 = data['uris'][0][1]\n",
    "    # 각각을 base64 인코딩해서 LLM 프롬프트에 삽입\n",
    "    with open(image_path_1, 'rb') as image_file:\n",
    "        image_data_1 = image_file.read()\n",
    "    inputs['image_data_1'] = base64.b64encode(image_data_1).decode('utf-8')\n",
    "    with open(image_path_2, 'rb') as image_file:\n",
    "        image_data_2 = image_file.read()\n",
    "    inputs['image_data_2'] = base64.b64encode(image_data_2).decode('utf-8')\n",
    "    return inputs\n",
    "\n",
    "def translate(text, target=\"english\"):\n",
    "    model  = ChatOpenAI(model='gpt-4.1', temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('system', f\"You are a translator. Translate the following text to {target}\"),\n",
    "            ('user', \"{text}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({'text' : text})\n",
    "\n",
    "# 실제 전체 동작 흐름 예시\n",
    "ko_query = \"바지하고 티셔츠를 파란색으로 추천\"                  # (1) 한글질문\n",
    "en_query = translate(ko_query, 'english')                       # (2) 영어 번역\n",
    "results = query_db(image_vdb, en_query)                         # (3) DB에서 top-2 유사 이미지 검색\n",
    "prompt_input = format_prompt_inputs(results, en_query)          # (4) 프롬프트 입력 포맷(이미지+문장)\n",
    "vision_chain = setup_chain()                                    # (5) LLM 체인 준비\n",
    "response_en = vision_chain.invoke(prompt_input)                 # (6) 패션 스타일링 추천 답변(영어)\n",
    "rt = translate(response_en, 'korean')                           # (7) 필요시 한글로 번역\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c822e5f",
   "metadata": {},
   "source": [
    "## 논문을 검색하는 agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77269c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ca0f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01551407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naver news\n",
    "loader = WebBaseLoader(\n",
    "    \"https://news.naver.com/\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "\n",
    "vectordb = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42098e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever, \"naver_news_search\", \"네이버 뉴스정보가 저장된 벡터 DB, 당일 뉴스 검색 가능\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
