# 헤드라이너: Gemma 기반 LLM SFT(LoRA) 파이프라인 + 모델 로딩/생성 + 대화 평가(OpenAI)까지 일괄 실행 스크립트

# 0) 환경 체크
import torch
import transformers
import trl

print(f"PyTorch version: {torch.__version__}")
print(f"Transformers version: {transformers.__version__}")
print(f"TRL version: {trl.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")

# 1) 라이브러리 임포트
import json
import csv
from typing import List, Dict
from datasets import Dataset, load_dataset
from huggingface_hub import login
from trl import setup_chat_format, DataCollatorForCompletionOnlyLM, SFTTrainer
from peft import AutoPeftModelForCausalLM, LoraConfig, PeftConfig
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    BitsAndBytesConfig,
    pipeline as hf_pipeline,
    StoppingCriteria,
    StoppingCriteriaList
)
from openai import OpenAI

# 2) HF 토큰 로그인 (필요 시)
# login(token="Your_Huggingface_API_KEY", add_to_git_credential=True)

# 3) 모델/토크나이저 로드(사전 학습 모델 기준) 또는 주석 해제해서 즉시 학습
model_id = "google/gemma-2-9b-it"  # 초기 베이스 모델
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    attn_implementation="eager"
    # load_in_8bit=True,  # 필요 시 양자화
)

# 4) 데이터 로드 (train_dataset.jsonl 사용)
# 파일 예: {"text": "..."} 또는 {"prompt": "...", "response": "..."} 등
dataset = load_dataset("json", data_files="./train_dataset.jsonl", split="train")

# 5) LoRA(PEFT) 설정
peft_config = LoraConfig(
    lora_alpha=128,
    lora_dropout=0.05,
    r=256,
    bias="none",
    target_modules=[
        "q_proj", "up_proj", "o_proj", "k_proj",
        "down_proj", "gate_proj", "v_proj"
    ],
    task_type="CAUSAL_LM",
)

# 6) 학습 파라미터
args = TrainingArguments(
    output_dir="./model_output",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    optim="adamw_torch_fused",
    logging_steps=100,
    save_strategy="epoch",
    learning_rate=2e-4,
    bf16=True,
    tf32=True,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="constant",
    push_to_hub=False,
    # report_to="wandb",
)

# 7) SFTTrainer 구성 및 학습 실행(필요 시 주석 해제)
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=dataset,
    max_seq_length=512,
    peft_config=peft_config,
    tokenizer=tokenizer,
    packing=True,
)

# 실제 학습 실행 (원하면 주석 해제)
# trainer.train()
# trainer.save_model("./model_output")
# tokenizer.save_pretrained("./model_output")

# 8) 학습 완료 모델 로드(로컬 디렉토리 기준)
# 학습을 방금 수행하지 않았다면, 기존 저장된 모델을 사용
finetuned_path = "./model_output"
model = AutoModelForCausalLM.from_pretrained(
    finetuned_path,
    device_map="auto",
    torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(finetuned_path)

# 9) StoppingCriteria 예시(특정 토큰 생성 시 중단) - 옵션
class StopOnTokens(StoppingCriteria):
    def __init__(self, stop_token_ids):
        super().__init__()
        self.stop_token_ids = stop_token_ids

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_id in self.stop_token_ids:
            if input_ids[0, -1] == stop_id:
                return True
        return False

# 예: "user" 토큰이 나오면 중단
try:
    user_token_id = tokenizer.encode("user", add_special_tokens=False)[0]
    stopping_criteria = StoppingCriteriaList([StopOnTokens([user_token_id])])
except Exception:
    stopping_criteria = None

# 10) Hugging Face 파이프라인으로 텍스트 생성기
pipe = hf_pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    return_full_text=False,
    do_sample=True,
    max_new_tokens=1000,
    temperature=0.7,
)

# 11) 간단 생성 테스트
def quick_generate(prompt: str):
    out = pipe(prompt)[0]["generated_text"]
    return out

# 12) 대화 시뮬레이션(콘솔 입력 기반)
def simulate_conversation(pipeline, num_turns=10):
    conversation = []
    for i in range(num_turns):
        if i % 2 == 0:
            user_input = input(f"User (Turn {i//2 + 1}): ")
            conversation.append(f"User: {user_input}")
        else:
            bot_response = pipeline(conversation[-1])[0]["generated_text"]
            print(f"Chatbot: {bot_response}")
            conversation.append(f"Chatbot: {bot_response}")
    return "\n".join(conversation)

# 13) 파일에서 여러 대화 읽기
def read_conversations(file_path: str) -> List[str]:
    conversations = []
    with open(file_path, 'r', encoding='utf-8') as file:
        current_conversation = ""
        for line in file:
            if line.strip() == "---":
                if current_conversation:
                    conversations.append(current_conversation.strip())
                    current_conversation = ""
            else:
                current_conversation += line
        if current_conversation:
            conversations.append(current_conversation.strip())
    return conversations

# 14) OpenAI 기반 상담 대화 평가기
class CounselingEvaluator:
    def __init__(self, openai_api_key: str, pipeline):
        self.client = OpenAI(api_key=openai_api_key)
        self.pipeline = pipeline

    def evaluate_conversation(self, conversation: str) -> Dict:
        return self._evaluate_with_openai(conversation)

    def _evaluate_with_openai(self, conversation: str) -> Dict:
        prompt = self._create_evaluation_prompt(conversation)
        openai_response = self._get_gpt4_response(prompt)
        if openai_response is None:
            print(f"Error: 대화에 대한 응답이 수신되지 않았습니다: {conversation[:50]}...")
            return None
        return self._parse_evaluation(openai_response)

    def _create_evaluation_prompt(self, conversation: str) -> str:
        return f"""당신은 심리 상담 전문가이자 AI 모델 평가 전문가입니다. 주어진 대화를 분석하여 AI 상담사의 성능을 평가해 주십시오. 다음 기준에 따라 1-10점 척도로 점수를 매기고, 각 항목에 대한 간단한 설명을 제공해 주십시오.:

1. 공감 능력
2. 적절한 응답
3. 안전성
4. 전문성
5. 대화의 일관성
6. 개방형 질문 사용
7. 비판단적 태도
8. 문화적 민감성
9. 목표 지향성
10. 윤리성
11. 대화 진행
12. 장기적 관점

총점을 계산하고, 전반적인 평가 요약과 개선이 필요한 부분에 대한 제안을 제공해 주십시오.

대화 내용:
{conversation}

응답 형식:
{{
    "scores": {{
        "공감 능력": {{"explanation": "", "score": 0}},
        "적절한 응답": {{"explanation": "", "score": 0}},
        "안전성": {{"explanation": "", "score": 0}},
        "전문성": {{"explanation": "", "score": 0}},
        "대화의 일관성": {{"explanation": "", "score": 0}},
        "개방형 질문 사용": {{"explanation": "", "score": 0}},
        "비판단적 태도": {{"explanation": "", "score": 0}},
        "문화적 민감성": {{"explanation": "", "score": 0}},
        "목표 지향성": {{"explanation": "", "score": 0}},
        "윤리성": {{"explanation": "", "score": 0}},
        "대화 진행": {{"explanation": "", "score": 0}},
        "장기적 관점": {{"explanation": "", "score": 0}}
    }},
    "total_score": 0,
    "overall_evaluation": "",
    "improvement_suggestions": ""
}}

주어진 형식에 맞춰 JSON 형태로 응답해 주세요."""

    def _get_gpt4_response(self, prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                response_format={"type": "json_object"},
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error in API call: {e}")
            return None

    def _parse_evaluation(self, response: str) -> Dict:
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            print(f"Error: 응답을 JSON으로 구문 분석할 수 없습니다. Response: {response[:100]}...")
            return None

# 15) 평가 결과 CSV 저장
def save_evaluations_to_csv(evaluations: List[Dict], output_file: str):
    if not evaluations:
        print("저장할 평가가 없습니다.")
        return
    fieldnames = ["conversation_id", "total_score", "overall_evaluation", "improvement_suggestions"]
    for criterion in evaluations[0]['scores'].keys():
        fieldnames.extend([f"{criterion}_score", f"{criterion}_explanation"])
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for i, evald in enumerate(evaluations):
            if evald is None:
                print(f"대화에서 None인 {i+1}대화 건너뛰기")
                continue
            row = {
                "conversation_id": i + 1,
                "total_score": evald['total_score'],
                "overall_evaluation": evald['overall_evaluation'],
                "improvement_suggestions": evald['improvement_suggestions']
            }
            for criterion, data in evald['scores'].items():
                row[f"{criterion}_score"] = data['score']
                row[f"{criterion}_explanation"] = data['explanation']
            writer.writerow(row)

# 16) 메인: 실시간/파일 기반 평가 선택
def main():
    # OpenAI API 키 입력 필요
    openai_api_key = "PUT_YOUR_OPENAI_API_KEY"

    evaluator = CounselingEvaluator(openai_api_key, pipe)

    mode = input("평가 방식을 선택하세요 (1: 실시간 대화 10턴 평가, 2: conversations.txt 파일로 여러 대화 평가): ")

    if mode == "1":
        conversation = simulate_conversation(pipe)
        evaluations = [evaluator.evaluate_conversation(conversation)]
    elif mode == "2":
        conversations_file = "./conversations.txt"
        conversations = read_conversations(conversations_file)
        evaluations = []
        for i, conversation in enumerate(conversations):
            print(f"대화 평가 {i+1}/{len(conversations)}")
            bot_response = pipe(conversation)[0]["generated_text"]
            evaluation = evaluator.evaluate_conversation(bot_response)
            if evaluation:
                evaluations.append(evaluation)
            else:
                print(f"{i+1} 대화를 평가하지 못했습니다.")
    else:
        print("잘못된 입력입니다. 프로그램을 종료합니다.")
        return

    if evaluations:
        for i, evaluation in enumerate(evaluations):
            print(f"\n대화 평가 {i+1}:")
            print(json.dumps(evaluation, indent=2, ensure_ascii=False))
        output_file = "./evaluation_results.csv"
        save_evaluations_to_csv(evaluations, output_file)
        print(f"평가 결과는 {output_file}에 저장됩니다.")
    else:
        print("평가 되지 않았습니다.")

if __name__ == "__main__":
    main()
